import asyncio
from playwright.async_api import async_playwright
import csv
import os

CHROME_PATH = "C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe"

async def scrape_explore_beauty(num_scrolls=100):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False, slow_mo=50, executable_path=CHROME_PATH)
        context = await browser.new_context()
        page = await context.new_page()

        await page.goto("https://www.tiktok.com/explore", timeout=60000)

        # ‚úÖ Click v√†o m·ª•c "ChƒÉm s√≥c s·∫Øc ƒë·∫πp"
        try:
            await page.wait_for_selector('span:has-text("Beauty Care")', timeout=10000)
            button = await page.query_selector('span:has-text("Beauty Care")')
            await button.click()
            await page.wait_for_timeout(3000)
        except Exception as e:
            print("‚ùå Kh√¥ng t√¨m th·∫•y n√∫t chuy√™n m·ª•c 'S·∫Øc ƒë·∫πp':", e)
            await browser.close()
            return []

        results = []

        for i in range(num_scrolls):
            print(f"\nüîÑ Scroll #{i+1}")
            await page.mouse.wheel(0, 1000)
            await page.wait_for_timeout(3000)

            cards = await page.query_selector_all('div[data-e2e="explore-card-desc"]')
            print(f"üîç Found {len(cards)} cards.")

            for card in cards:
                try:
                    link_elem = await card.query_selector('a[href*="/@"]')
                    if not link_elem:
                        continue

                    href = await link_elem.get_attribute("href")
                    if href:
                        username = href.split("/@")[-1]
                        results.append({
                            "user_name": username,
                            "topic": "Beauty"
                        })
                except Exception as e:
                    print("‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω 1 card:", e)

        await browser.close()
        return results

def save_to_csv(data, filename='D:/Downloads/beauty_users_crawl2.csv'):
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    with open(filename, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["user_name", "topic"])
        writer.writeheader()
        writer.writerows(data)
    print(f"‚úÖ ƒê√£ l∆∞u {len(data)} users v√†o {filename}")

if __name__ == "__main__":
    try:
        data = asyncio.run(scrape_explore_beauty(num_scrolls=150))
        if data:
            unique_users = {item['user_name']: item for item in data}.values()
            save_to_csv(list(unique_users))
            print(f"üßÆ T·ªïng s·ªë user: {len(data)} | Unique: {len(unique_users)}")
        else:
            print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu.")
    except Exception as e:
        print("‚ùóL·ªói to√†n c·ª•c:", e)

